{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is holds helper functions for reading in, and processing, data files exported from mocap experiments from the Qualysis system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import scipy.signal as scisig\n",
    "from scipy.stats import norm\n",
    "from scipy.signal import butter, sosfreqz, sosfiltfilt, boxcar, savgol_filter, find_peaks\n",
    "from scipy.signal.signaltools import hilbert\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the default figure sizes.\n",
    "#\n",
    "matplotlib.rc('figure', figsize=(15, 7))\n",
    "sns.set(rc={'figure.figsize':(15,7)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_participant_data_from_path(filepath, debug=False):\n",
    "    \n",
    "    '''\n",
    "        Loads data in from Qualysis exported (TSV) files, and returns a multi-index \n",
    "        dataframe.\n",
    "        \n",
    "        _____ INPUTS _____\n",
    "        'filepath': Directory path to the data to load in (e.g. \"./Data/Train\")\n",
    "        \n",
    "        _____ OUTPUTS _____\n",
    "        'data_df': A multi-index dataframe that has been populated by all data found\n",
    "                   in the filepath.\n",
    "    '''\n",
    "    \n",
    "    files = os.listdir(filepath)\n",
    "    \n",
    "    df_list = []\n",
    "    for file in files:\n",
    "        data_file = os.path.join(filepath, file)\n",
    "        print(\"Reading in data from: \", data_file)\n",
    "\n",
    "        # Grab attributes about the data that we will use to form the dataframe.\n",
    "        #\n",
    "        attrs = {}\n",
    "        marker_names = []\n",
    "        skip_cnt = 0\n",
    "        with open(data_file) as fd:\n",
    "            rd = csv.reader(fd, delimiter='\\t')\n",
    "            for i,row in enumerate(rd):\n",
    "                # NOTE: \n",
    "                # - We always remove the first item in the 'row', as it's the name of the\n",
    "                #   attribute we are interested in (e.g. 'MARKER_NAMES'), not the \n",
    "                #   value itself.\n",
    "                #\n",
    "                if \"FREQUENCY\" in row:\n",
    "                    attrs['frequency'] = float(row[1:][0])\n",
    "                    if debug:\n",
    "                        print(\"Found sampling frequency\\n\", row[1:])\n",
    "                if \"DESCRIPTION\" in row:\n",
    "                    attrs['description'] = row[1].split(\" - \")\n",
    "                    if debug:\n",
    "                        print(\"Found description\\n\", row[1:])\n",
    "                if \"TIME_STAMP\" in row:\n",
    "                    date = \"\".join(row[1].split(\",\"))\n",
    "                    attrs['date'] = date\n",
    "                    if debug:\n",
    "                        print(\"Found time stamp\\n\", row[1:])\n",
    "                        print(date)\n",
    "                if \"MARKER_NAMES\" in row:\n",
    "                    if debug:\n",
    "                        print(\"Found marker names\\n\", row[1:])\n",
    "                    attrs['marker_names'] = row[1:]\n",
    "                    skip_cnt = i+1\n",
    "                    break\n",
    "\n",
    "        # Create x,y,z names for each marker so we can populate the column names in the\n",
    "        # dataframe correctly.\n",
    "        # (e.g. 'hip_left' becomes 'hip_left_x', 'hip_left_y', 'hip_left_z')\n",
    "        #\n",
    "        marker_data_names = []\n",
    "        for name in attrs['marker_names']:\n",
    "            xyz = [name + \"_x\", name + \"_y\", name + \"_z\"]\n",
    "            marker_data_names.extend(xyz)\n",
    "\n",
    "        # Assign the new names to our attributes dictionary.\n",
    "        #\n",
    "        attrs['marker_data_names'] = marker_data_names\n",
    "        if debug:\n",
    "            print('\\n', marker_data_names)\n",
    "\n",
    "        # Read in the data and form the dataframe.\n",
    "        #\n",
    "        # df = pd.read_csv(data_file, sep='\\t', names=attrs['marker_data_names'], skiprows=i+1, header=None)\n",
    "        df = pd.read_csv(data_file, sep='\\t', names=attrs['marker_data_names'], skiprows=skip_cnt+1, header=None)    \n",
    "\n",
    "        # Create a multi-index so we can later group data easily.\n",
    "        #\n",
    "        cnt = len(df) # How many data points\n",
    "        _participant = attrs['description'][0]\n",
    "        _type = attrs['description'][1]\n",
    "        _point_perf = attrs['description'][2]\n",
    "        levels = [[_participant], [_type] , [_point_perf]]\n",
    "        labels = [[0]*cnt for _ in levels]\n",
    "        names = ['participant', 'type', 'point-perf.']\n",
    "        index = pd.MultiIndex(levels=levels, labels=labels, names=names)\n",
    "        df.index = index\n",
    "\n",
    "\n",
    "        freq = str(1/(attrs['frequency'])) + \"S\"\n",
    "        df['time'] = pd.DatetimeIndex(start=attrs['date'], periods=len(df), freq=freq)\n",
    "        df.set_index('time', append=True, inplace=True)\n",
    "\n",
    "        df_list.append(df)\n",
    "\n",
    "    # Form the final dataframe that contains all data read in.\n",
    "    #\n",
    "    data_df = pd.concat(df_list)\n",
    "    \n",
    "    return data_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_spine_distance(df):\n",
    "    ''''\n",
    "        Calculate the Euclidean distance of each segment of the spine markers over all time of the \n",
    "        experiment. (i.e. markers s0->s1, s1->s2, s2->s3)\n",
    "        \n",
    "        _____ INPUTS _____\n",
    "        'df': Dataframe containing columns that represent the individual spine markers in 3-d space.\n",
    "        \n",
    "        _____ OUTPUTS _____\n",
    "        'dist_df': Dataframe containing the Euclidean distances that connects each marker to it's previous.\n",
    "                   (i.e. the distance between s0->s1, s1->s2, etc.)\n",
    "    '''\n",
    "    \n",
    "    dist_df = pd.DataFrame()\n",
    "    # SegmentN is the distance from spine marker 0 (i.e. s_N) to the next \n",
    "    # spine marker (i.e. s_N+1).\n",
    "    #\n",
    "    dist_df['segment0'] = np.sqrt((df['spine_0_x'] - df['spine_1_x'])**2 + \n",
    "                            (df['spine_0_y'] - df['spine_1_y'])**2 +\n",
    "                            (df['spine_0_z'] - df['spine_1_z'])**2)\n",
    "    dist_df['segment1'] = np.sqrt((df['spine_1_x'] - df['spine_2_x'])**2 + \n",
    "                            (df['spine_1_y'] - df['spine_2_y'])**2 +\n",
    "                            (df['spine_1_z'] - df['spine_2_z'])**2)\n",
    "    dist_df['segment2'] = np.sqrt((df['spine_2_x'] - df['spine_3_x'])**2 + \n",
    "                            (df['spine_2_y'] - df['spine_3_y'])**2 +\n",
    "                            (df['spine_2_z'] - df['spine_3_z'])**2)\n",
    "    dist_df['dist_total'] = dist_df.sum(axis=1)\n",
    "    \n",
    "    return dist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_to_range(x, lower, upper):\n",
    "    '''\n",
    "        Scale data between a range defined by [lower, upper]\n",
    "        \n",
    "        _____ INPUTS _____\n",
    "        'x': Array of data.\n",
    "        'lower': Value of lower bound.\n",
    "        'upper': Value of upper bound.\n",
    "        \n",
    "        _____ OUTPUTS _____\n",
    "        Returns array scaled to sit between the range [lower, upper].\n",
    "    '''\n",
    "    return (x-min(x))*(upper-lower)/(max(x)-min(x)) + lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    return (x - x.mean())/x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    return (x - x.min())/(x.max()-x.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of filtering routines.\n",
    "#\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    # Return second-order-sections for numerical stability.\n",
    "    #\n",
    "    sos = butter(N=order, Wn=[low, high], analog=False, btype='bandpass', output='sos')\n",
    "    return sos\n",
    "\n",
    "def butter_bandpass_filter(signal, lowcut, highcut, fs, order=5):\n",
    "    sos = butter_bandpass(lowcut, highcut, fs, order)\n",
    "    filtered_signal = sosfiltfilt(sos, signal)\n",
    "    return filtered_signal\n",
    "\n",
    "def butter_lowpass(cutoff, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    cutoff_freq = cutoff / nyq\n",
    "    # Return second-order-sections for numerical stability.\n",
    "    #\n",
    "    sos = butter(N=order, Wn=cutoff_freq, analog=False, btype='lowpass', output='sos')\n",
    "    return sos\n",
    "\n",
    "def butter_lowpass_filter(signal, cutoff, fs, order=5):\n",
    "#     b, a = butter_lowpass(cutoff, fs, order)\n",
    "    sos = butter_lowpass(cutoff, fs, order)\n",
    "    filtered_signal = sosfiltfilt(sos, signal)\n",
    "    return filtered_signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of smoothing routines.\n",
    "#\n",
    "\n",
    "def boxcar_smooth(signal, window):\n",
    "    # 'window': length of points\n",
    "    \n",
    "    return np.convolve(signal, boxcar(M=window))\n",
    "\n",
    "def savgol_smooth(signal, window, order=3):\n",
    "    # 'window': length of points\n",
    "    #           NOTE: window must be odd.\n",
    "    # 'order': polynomial order \n",
    "        \n",
    "    if np.mod(window, 2) == 0:\n",
    "        print(\"Window must be odd. Incrementing by 1\")\n",
    "        window += 1\n",
    "    return savgol_filter(signal, window, order)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow_py35)",
   "language": "python",
   "name": "tensorflow_py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
